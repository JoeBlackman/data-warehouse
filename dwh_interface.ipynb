{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AWS DATA WAREHOUSE INTERFACE**\n",
    "\n",
    "User will run code cells in this notebook to set the state of the data warehouse\n",
    "- PREREQUISITE: IMPORT LIBRARIES\n",
    "- COLLECT CLUSTER CONFIGURATION\n",
    "- CONFIGURE ROLE AND POLICY\n",
    "- CREATE REDSHIFT CLUSTER\n",
    "- CONFIGURE REDSHIFT CLUSTER\n",
    "- GET CLUSTER DESCRIPTION\n",
    "- DELETE REDSHIFT CLUSTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PREREQUISITE: IMPORT LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusable Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **COLLECT CLUSTER CONFIGURATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = configparser.ConfigParser()\n",
    "#config.read('dwh.cfg')\n",
    "\n",
    "#REGION = config.get('AWS', 'REGION')\n",
    "#KEY = config.get('AWS', 'KEY')\n",
    "#SECRET = config.get('AWS', 'SECRET')\n",
    "#CLUSTER_TYPE = config.get('REDSHIFT', 'DWH_CLUSTER_TYPE')\n",
    "#NUM_NODES = config.get('REDSHIFT', 'DWH_NUM_NODES')\n",
    "#NODE_TYPE = config.get('REDSHIFT', 'DWH_NODE_TYPE')\n",
    "#CLUSTER_ID = config.get('REDSHIFT', 'DWH_CLUSTER_IDENTIFIER')\n",
    "#DB_NAME = config.get('REDSHIFT', 'DWH_DB')\n",
    "#USER = config.get('REDSHIFT', 'DWH_DB_USER')\n",
    "#PW = config.get('REDSHIFT', 'DWH_DB_PASSWORD')\n",
    "#PORT = config.get('REDSHIFT', 'DWH_PORT')\n",
    "#IAM_ROLE = config.get('REDSHIFT', 'DWH_IAM_ROLE_NAME')\n",
    "#IAM_ROLE_ARN = config.get('IAM_ROLE', 'ARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CONFIGURE ROLE AND POLICY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\n",
    "    'iam',\n",
    "    aws_access_key_id=config.KEY,\n",
    "    aws_secret_access_key=config.SECRET,\n",
    "    region_name=config.REGION\n",
    ")\n",
    "print(config.KEY)\n",
    "print(config.SECRET)\n",
    "print(config.REGION)\n",
    "\n",
    "try:\n",
    "    print(\"Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=config.IAM_ROLE,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {\n",
    "                'Statement': [\n",
    "                    {\n",
    "                        'Action': 'sts:AssumeRole',\n",
    "                        'Effect': 'Allow',\n",
    "                        'Principal': \n",
    "                        {\n",
    "                            'Service': 'redshift.amazonaws.com'\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                'Version': '2012-10-17'\n",
    "            }\n",
    "        )\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"Attaching policy to new role\")\n",
    "iam.attach_role_policy(\n",
    "    RoleName=config.IAM_ROLE,\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    ")['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "roleArn = iam.get_role(RoleName=config.IAM_ROLE)['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CREATE REDSHIFT CLUSTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_client = boto3.client(\n",
    "    'redshift',\n",
    "    region_name=config.REGION,\n",
    "    aws_access_key_id=config.KEY,\n",
    "    aws_secret_access_key=config.SECRET\n",
    ")\n",
    "\n",
    "try:\n",
    "    redshift_client.create_cluster(\n",
    "        ClusterType=config.CLUSTER_TYPE,\n",
    "        NodeType=config.NODE_TYPE,\n",
    "        NumberOfNodes=int(config.NUM_NODES),\n",
    "        DBName=config.DB_NAME,\n",
    "        ClusterIdentifier=config.CLUSTER_ID,\n",
    "        MasterUsername=config.USER,\n",
    "        MasterUserPassword=config.PW,\n",
    "        IamRoles=[roleArn]\n",
    "    )\n",
    "    print(redshift_client.describe_clusters(ClusterIdentifier=config.CLUSTER_ID)['Clusters'][0])\n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GET CLUSTER DESCRIPTION**\n",
    "\n",
    "This will be called repeatedly in this notebook. It should be idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift_client.describe_clusters(ClusterIdentifier=config.CLUSTER_ID)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwh_endpoint = myClusterProps['Endpoint']['Address']\n",
    "dwh_role_arn = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", dwh_endpoint)\n",
    "print(\"DWH_ROLE_ARN :: \", dwh_role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config.ENDPOINT = dwh_endpoint\n",
    "config.IAM_ROLE_ARN = dwh_role_arn\n",
    "\n",
    "cfg = configparser.ConfigParser()\n",
    "cfg.read('dwh.cfg')\n",
    "cfg.set('REDSHIFT', 'DWH_ENDPOINT', dwh_endpoint)\n",
    "cfg.set('IAM_ROLE', 'ARN', dwh_role_arn)\n",
    "with open('dwh.cfg', 'w') as configfile:\n",
    "    cfg.write(configfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CONFIGURE REDSHIFT CLUSTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.resource(\n",
    "    'ec2',\n",
    "    region_name=config.REGION,\n",
    "    aws_access_key_id=config.KEY,\n",
    "    aws_secret_access_key=config.SECRET\n",
    ")\n",
    "\n",
    "cluster_props = redshift_client.describe_clusters(ClusterIdentifier=config.CLUSTER_ID)['Clusters'][0]\n",
    "vpc = ec2.Vpc(id=cluster_props['VpcId'])\n",
    "defaultSg = list(vpc.security_groups.all())[0]\n",
    "defaultSg.authorize_ingress(\n",
    "    GroupName=defaultSg.group_name,\n",
    "    CidrIp='0.0.0.0/0',\n",
    "    IpProtocol='TCP',\n",
    "    FromPort=int(config.PORT),\n",
    "    ToPort=int(config.PORT)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **INSPECT REDSHIFT DATABASE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(f\"\"\"\n",
    "        host={config.ENDPOINT} \n",
    "        dbname={config.DB_NAME} \n",
    "        user={config.USER} \n",
    "        password={config.PW} \n",
    "        port={config.PORT}\n",
    "\"\"\")\n",
    "cur = conn.cursor()\n",
    "q = \"\"\"\n",
    "SELECT DISTINCT tablename\n",
    "FROM PG_TABLE_DEF\n",
    "WHERE schemaname='public';\n",
    "\"\"\"\n",
    "cur.execute(q)\n",
    "print(cur.fetchall())\n",
    "#pd.DataFrame(data = cur.fetchall)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(config.USER, config.PW, config.ENDPOINT, config.PORT, config.DB_NAME)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM stl_load_errors LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM staging_events LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM staging_songs LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM songs LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM artists LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM users LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM time LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM songplays LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "WITH uniq_staging_events AS (\n",
    "        SELECT userId, firstName, lastName, gender, level, ROW_NUMBER() OVER(PARTITION BY userId ORDER BY ts DESC) AS rank\n",
    "        FROM staging_events\n",
    "        WHERE userId != NULL\n",
    "    )\n",
    "SELECT userId, firstName, lastName, gender, level\n",
    "FROM uniq_staging_events\n",
    "WHERE rank = 1;\n",
    "\"\"\"\n",
    "%sql :q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = %sql SELECT * FROM staging_events WHERE userId = 69 LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\n",
    "        'artist', 'auth', 'firstName', 'gender', 'itemInSession', 'lastName',\n",
    "        'length', 'location', 'method', 'page', 'registration',\n",
    "        'sessionId', 'song', 'status', 'ts', 'userAgent', 'userId', 'level'\n",
    "    ]\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "conn = psycopg2.connect(f\"\"\"\n",
    "        host={config.ENDPOINT}\n",
    "        dbname={config.DB_NAME}\n",
    "        user={config.USER}\n",
    "        password={config.PW}\n",
    "        port={config.PORT}\n",
    "    \"\"\")\n",
    "cur = conn.cursor()\n",
    "cur.execute('SELECT * FROM staging_events;')\n",
    "staging_events_data = cur.fetchall()\n",
    "staging_events_df = pd.DataFrame(\n",
    "    staging_events_data,\n",
    "    columns=[\n",
    "        'artist', 'auth', 'firstName', 'gender', 'itemInSession', 'lastName',\n",
    "        'length', 'level', 'location', 'method', 'page', 'registration',\n",
    "        'sessionId', 'song', 'status', 'ts', 'userAgent', 'userId'\n",
    "    ]\n",
    ")\n",
    "for index, row in staging_events_df.iterrows():\n",
    "    if row['firstName'] == \"Anabelle\":\n",
    "        print(row)\n",
    "        print(row['userId'])\n",
    "        check = None\n",
    "        if type(row['userId']) == float:\n",
    "            check = int(row['userId'])\n",
    "        if type(row['userId']) == str:\n",
    "            check = int(float(row['userId']))\n",
    "        print(check)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DELETE REDSHIFT CLUSTER**\n",
    "\n",
    "The next cell will delete the cluster. Make sure to run the cell to get cluster description after deleting the cluster to confirm its deletion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    redshift_client.delete_cluster(ClusterIdentifier=config.CLUSTER_ID, SkipFinalClusterSnapshot=True)\n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift_client.describe_clusters(ClusterIdentifier=config.CLUSTER_ID)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.detach_role_policy(RoleName=config.IAM_ROLE, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=config.IAM_ROLE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f340908d974e6e642b0f24874110ca1fcf85e3e8e95ea0f11995d54a92a2f68"
  },
  "kernelspec": {
   "display_name": "data_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
